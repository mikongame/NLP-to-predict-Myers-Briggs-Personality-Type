{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://bit.ly/2VnXWr2\" width=\"100\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project | ML: Job offers' Fraud-Detection with NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to experience how to work in a ML project as a group, and to learn more on NLP we have been working on this dataset from Kaggle, [[Real or Fake] Fake Job Posting Prediction](https://www.kaggle.com/shivamb/real-or-fake-fake-jobposting-prediction), that holds around 18K job descriptions out of which about 900 are fake. The data consists of both textual information and meta-information about the jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We mainly wanted to create a **classification model using text data features and meta-features to predict which job descriptions are fraudulent**. As well as, finding out if there are **key traits/features** (words, entities, phrases) of job descriptions which are **intrinsically fraudulent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-08T18:38:59.933692Z",
     "start_time": "2020-05-08T18:38:55.208330Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Data Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Visualization for text\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import os\n",
    "import random\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Text Processing\n",
    "import re\n",
    "import itertools\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "\n",
    "# Machine Learning packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore noise warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Work with pickles\n",
    "import pickle\n",
    "\n",
    "# Fix imbalance\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "\n",
    "# Model training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning of the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Althought the metrics of the different models are really good, we can still improve the performance of the models. Therefore, a fine tunning of the different parameters of each models has to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T10:48:29.190167Z",
     "start_time": "2020-05-07T10:48:29.185180Z"
    }
   },
   "source": [
    "<img src=\"images/danger-explosives-sign.jpg\" width=\"200\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T18:17:55.575064Z",
     "start_time": "2020-05-07T18:17:55.570099Z"
    }
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "Stop right there! The following cells takes some time to complete.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m Stop right there! The following cells takes some time to complete.\n"
     ]
    }
   ],
   "source": [
    "raise SystemExit(\"Stop right there! The following cells takes some time to complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:51:01.638220Z",
     "start_time": "2020-05-07T16:50:16.083Z"
    }
   },
   "source": [
    "### KNN Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new a knn model\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "#create a dictionary of all values we want to test for n_neighbors\n",
    "params_knn = {'n_neighbors': np.arange(1, 25), \n",
    "              'weights': ['uniform', 'distance'], \n",
    "              'algorithm': ['auto', 'kd_tree', 'brute'],\n",
    "              'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski'],\n",
    "            }\n",
    "\n",
    "#knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
    "\n",
    "#knn_gs.fit(X_train, y_train)\n",
    "\n",
    "#knn_best = knn_gs.best_estimator_\n",
    "\n",
    "#print(knn_gs.best_params_)\n",
    "\n",
    "\n",
    "\"\"\"The code was executed in  google colab, the result is \n",
    "\n",
    "{'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 21, 'weights': 'distance'}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Without Tuning\"\"\"\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "y_pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" With Tuning \"\"\"\n",
    "\n",
    "knn = KNeighborsClassifier(algorithm =  'auto', metric = 'manhattan', n_neighbors= 21, weights = 'distance')\n",
    "y_pred = knn.fit(X_train, y_train).predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:51:01.640215Z",
     "start_time": "2020-05-07T16:50:16.086Z"
    }
   },
   "source": [
    "### RandomForest Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "params_rfc = { \n",
    "                'n_estimators' : np.arange(50,250),\n",
    "                'criterion' : ['gini','entropy'],\n",
    "                'max_features' : ['sqrt','log2']\n",
    "            }\n",
    "\n",
    "#rfc_gs = GridSearchCV(rfc, params_rfc, cv=5)\n",
    "\n",
    "#rfc_gs.fit(X_train, y_train)\n",
    "\n",
    "#rfc_best = rfc_gs.best_estimator_\n",
    "\n",
    "#print(rfc_gs.best_params_)\n",
    "\n",
    "\"\"\"\n",
    "The code was executed in  google colab, the result is\n",
    "'criterion': 'entropy', 'max_features': 'sqrt', 'n_estimators': 163\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Without Tuning \"\"\"\n",
    "\n",
    "rfc = RandomForestClassifier(random_state = 42)\n",
    "y_pred = rfc.fit(X_train, y_train).predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" With Tuning \"\"\"\n",
    "\n",
    "rfc = RandomForestClassifier(random_state = 42, criterion = 'entropy', max_features = 'sqrt', n_estimators = 163)\n",
    "y_pred = rfc.fit(X_train, y_train).predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "params_dtc = { \n",
    "                'class_weight' : ['balanced', None],\n",
    "                'criterion' : ['gini','entropy'],\n",
    "                'spliter' : ['random','best']\n",
    "                'max_features' : ['sqrt','log2']\n",
    "            }\n",
    "\n",
    "#dtc_gs = GridSearchCV(dtc, params_dtc, cv=5)\n",
    "\n",
    "#dtc_gs.fit(X_train, y_train)\n",
    "\n",
    "#dtc_best = dtc_gs.best_estimator_\n",
    "\n",
    "#print(dtc_gs.best_params_)\n",
    "\n",
    "\"\"\"\n",
    "The code was executed in  google colab, the result is\n",
    "{'class_weight': 'balanced', 'criterion': 'gini', 'max_features': 'sqrt', 'splitter': 'best'}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T16:51:01.641212Z",
     "start_time": "2020-05-07T16:50:16.089Z"
    }
   },
   "source": [
    "### MLP Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter = 10000, hidden_layer_sizes = (300,))\n",
    "\n",
    "params_mlp = {\n",
    "              'activation': ['identity', 'logistic', 'tanh', 'relu'], \n",
    "              'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "              'learning_rate': ['constant', 'invscaling', 'adaptive', 'minkowski'],\n",
    "            }\n",
    "\n",
    "#mlp_gs = GridSearchCV(mlp, params_mlp, cv=5)\n",
    "\n",
    "#mlp_gs.fit(X_train, y_train)\n",
    "\n",
    "#mlp_best = mlp_gs.best_estimator_\n",
    "\n",
    "#print(knn_gs.best_params_)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The code was executed in  google colab, the result is\n",
    "\n",
    "{'activation': 'tanh', 'learning_rate': 'constant', 'solver': 'lbfgs'}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Without Tuning \"\"\"\n",
    "\n",
    "mlp = MLPClassifier(max_iter = 10000,hidden_layer_sizes = (300,))\n",
    "y_pred = mlp.fit(X_train, y_train).predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" With  Tuning \"\"\"\n",
    "\n",
    "mlp = MLPClassifier(max_iter = 10000,hidden_layer_sizes = (300,),activation='tanh',learning_rate='constant',solver='lbfgs')\n",
    "y_pred = mlp.fit(X_train, y_train).predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_report(model, X_train, X_test, y_train, y_test, name):\n",
    "    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    accuracy     = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='accuracy'))\n",
    "    precision    = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='precision'))\n",
    "    recall       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='recall'))\n",
    "    f1score      = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='f1'))\n",
    "    rocauc       = np.mean(cross_val_score(model, X_train, y_train, cv=strat_k_fold, scoring='roc_auc'))\n",
    "    y_pred = model.predict(X_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_pred, y_test).ravel()\n",
    "    specificity = tn / (tn+fp)\n",
    "    \n",
    "\n",
    "    df_model = pd.DataFrame({'model'        : [name],\n",
    "                             'accuracy'     : [accuracy],\n",
    "                             'precision'    : [precision],\n",
    "                             'recall'       : [recall],\n",
    "                             'f1score'      : [f1score],\n",
    "                             'rocauc'       : [rocauc],\n",
    "                             'specificity': [specificity]\n",
    "                            })  \n",
    "    return df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "          'knn': KNeighborsClassifier(algorithm =  'auto', metric = 'manhattan', n_neighbors= 21, weights = 'distance'),\n",
    "          'decisiontree': DecisionTreeClassifier(random_state = 42, class_weight= None, criterion='entropy', max_features='sqrt', splitter='best'),\n",
    "          'randomforest': RandomForestClassifier(random_state = 42, criterion = 'entropy', max_features = 'sqrt', n_estimators = 163),\n",
    "           'MLP': MLPClassifier(max_iter = 10000,hidden_layer_sizes = (300,),activation='tanh',learning_rate='constant',solver='lbfgs')\n",
    "         }\n",
    "\n",
    "models_df = pd.concat([baseline_report(model, X_train, X_test, y_train, y_test, name) for (name, model) in models.items()])\n",
    "models_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "347.4px",
    "left": "981px",
    "right": "20px",
    "top": "120px",
    "width": "477.4px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
